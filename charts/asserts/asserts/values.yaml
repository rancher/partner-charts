## Global parameters
##
## This will override any available parameters in this chart
## as well as dependent charts
##
## Current available global parameters: storageClass, secureCookie
global:
  storageClass: ""
  # set to true if oauth is configured on Asserts and exclusively using https
  secureCookie: false

nameOverride: ""
fullnameOverride: ""
clusterDomain: svc.cluster.local

rbac:
  ## Namespaced role and rolebinding
  ## to allow for get/list/watch of
  ## pod, endpoints, and services in the
  ## installed {{ .Release.Namespace }}
  create: true
  annotations: {}
  extraLabels: {}

## ServiceAccount configuration
##
serviceAccount:
  create: true
  ## name of the service account to use.
  ## note that the values in this file assume all the service
  ## accounts for dependent charts are using the "asserts"
  ## service account. This is here for completness and potential
  ## future changes.
  name: ""
  imagePullSecrets: []
  annotations: {}
  extraLabels: {}

## If Prometheus-Operator is used (detected by checking "monitoring.coreos.com/v1" api existence)
## Then they will be created, else the Asserts Tsdb will scrape the services
##
serviceMonitor:
  enabled: true
  # endpoints and appropriate relabelings
  # relabelings used to avoid duplicates
  # this allows us to avoid errors and keep everything in one
  # ServiceMonitor
  endpoints:
    - port: http
      path: /api-server/actuator/prometheus
      relabelings:
        - sourceLabels: [job]
          regex: "{{.Release.Name}}-server"
          action: keep
    - port: http
      path: /authorization/actuator/prometheus
      relabelings:
        - sourceLabels: [job]
          regex: "{{.Release.Name}}-authorization"
          action: keep
    - port: http
      path: /metrics
      relabelings:
        - sourceLabels: [job]
          regex: "{{.Release.Name}}-server"
          action: drop
        - sourceLabels: [job]
          regex: "{{.Release.Name}}-ui"
          action: drop
        - sourceLabels: [job]
          regex: "{{.Release.Name}}-alertmanager-headless"
          action: drop
    - port: http-metrics
      path: /metrics
  # Use if you need to add a label that matches
  # your prometheus-operator serviceMonitorSelector (e.g. release: kube-prometheus-stack)
  extraLabels: {}

## Asserts cluster env and site
##
## IGNORE IF RUNNING Promtheus-Operator!!!
##
## This is required in order for Asserts to scrape a
## and monitor itself. This should be set to the env and site
## of the cluster asserts is being installed in.
## This means that the env and site of the datasource
## for this cluster (set in the Asserts UI), should
## match these values.
assertsClusterEnv: ""
assertsClusterSite: ""

## Asserts Url
##
## Used to configure where your notifications
## will link back to in Asserts (see /config/alertmanager-configmap.yaml )
## If setting ui.ingress.enabled: true (similarly a node ip if using NodePort)
## should match the hostname used in addtion to
## adding the desired scheme (http/https)
##
## If unset, leave the url like this as opposed to empty
## as it will work when using port-forwarding.
assertsUrl: http://localhost:8080

## Receiver configuration
## the notification receiver (e.g. slack, pagerduty, etc..)
## wait times/intervals
receiver:
  group_wait: 1m
  group_interval: 15m
  repeat_interval: 30m

## Slack configuration
## template is generated from alertmanager-templates/_slack.tpl
## when slack is enabled
##
slack:
  enabled: false
  # api_url: https://hooks.slack.com/some-url
  # channel: "#alerts"

## PagerDuty configuration
## template is generated from alertmanager-templates/_pagerduty.tpl
## when pagerduty is enabled
##
## ref: https://www.pagerduty.com/docs/guides/prometheus-integration-guide/
pagerduty:
  enabled: false
  # routing_key: <your-events-api-v2-integration-key>
  # url: https://events.pagerduty.com/v2/enqueue

## Asserts server configuration
##
server:
  nameOverride: ""
  fullnameOverride: ""

  replicaCount: 1

  image:
    repository: asserts/asserts-server
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v0.2.415

  initContainers:
    - name: wait-for-postgres
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Values.postgres.fullnameOverride}}.{{.Release.Namespace}}.{{.Values.clusterDomain}}:5432"
        - "-t"
        - "420"

  imagePullSecrets: []

  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}

  service:
    type: ClusterIP
    port: 8030

  resources: {}

  ## wait 30 seconds before pod termination
  ## to allow application shut down
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle
  terminationGracePeriodSeconds: 30

  ## environment variables to add to the asserts-server pod
  extraEnv: []

  ## environment variables from secrets or configmaps to add to the asserts-server pod
  extraEnvFrom: []

  annotations: {}

  extraLabels: {}

  extraPodLabels: {}

  extraPodAnnotations: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraContainers: []

  extraVolumeMounts: []

  extraVolumes: []

  dataPath: /opt/asserts/data

  persistence:
    enabled: true

    ## Persistent Volume storage class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
    storageClass: ""

    ## Persistent Volume access modes
    accessModes:
      - ReadWriteOnce

    ## Persistent Volume size
    size: 8Gi

    ## When set, will use the existing PVC for persistence
    existingClaim: ""

## Asserts authorization server configuration
##
authorization:
  nameOverride: ""
  fullnameOverride: ""

  replicaCount: 1

  image:
    repository: asserts/authorization
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v0.2.415

  initContainers:
    - name: wait-for-postgres
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Values.postgres.fullnameOverride}}.{{.Release.Namespace}}.{{.Values.clusterDomain}}:5432"
        - "-t"
        - "420"

  imagePullSecrets: []

  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}

  service:
    type: ClusterIP
    port: 8070

  resources: {}

  ## environment variables to add to the asserts-server pod
  extraEnv: []

  ## environment variables from secrets or configmaps to add to the asserts-server pod
  extraEnvFrom: []

  annotations: {}

  extraLabels: {}

  extraPodLabels: {}

  extraPodAnnotations: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraVolumeMounts: []

  extraVolumes: []

## Asserts ui configuration
##
ui:
  nameOverride: ""
  fullnameOverride: ""

  replicaCount: 1

  image:
    repository: asserts/asserts-ui
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v0.1.1093

  imagePullSecrets: []

  service:
    type: ClusterIP
    port: 8080
    annotations: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ingress:
    enabled: false

    ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    extraLabels: {}

    hosts: []
    #   - asserts-ui.domain.com

    path: /

    ## pathType is only for k8s >= 1.18
    pathType: Prefix

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    tls: []
    #   - secretName: asserts-ui-tls
    #     hosts:
    #       - asserts-ui.domain.com

  resources: {}

  annotations: {}

  ## environment variables to add to the asserts-ui pod
  extraEnv: []
  ## environment variables from secrets or configmaps to add to the asserts-ui pod
  extraEnvFrom: []
  #   - secretRef:
  #       name: license

  extraLabels: {}

  extraPodLabels: {}

  podAnnotations: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraContainers: []

  extraVolumeMounts: []

  extraVolumes: []


## Asserts grafana configuration
##
grafana:
  nameOverride: ""
  fullnameOverride: ""

  image:
    repository: asserts/grafana
    pullPolicy: IfNotPresent
    ## Overrides the image tag whose default is the chart appVersion.
    tag: v1.0.180

  imagePullSecrets: []

  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}

  service:
    type: ClusterIP
    port: 3000
    annotations: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ingress:
    enabled: false

    ## For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    extraLabels: {}

    hosts: []
    #   - asserts-grafana.domain.com

    path: /

    ## pathType is only for k8s >= 1.18
    pathType: Prefix

    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation

    tls: []
    #   - secretName: asserts-grafana-tls
    #     hosts:
    #       - asserts-grafana.domain.com

  securityContext:
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001

  resources: {}

  nodeSelector: {}

  tolerations: []

  affinity: {}

  extraVolumeMounts: []

  extraVolumes: []

  ## Grafana admin password configuration
  ##
  ## If left with password: "" and existingSecret: ""
  ## a random alpha numeric password will be generated on
  ## first run. Upon upgrading the release, the credentials
  ## stored in the secret will be retrieved, secret handling
  ## is out of the box. Note that this will fail if using the
  ## "helm-diff" plugin and would require setting it explicitly
  ## or providing an existingSecret.
  ##
  ## ref: https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases
  auth:
    password: ""
    existingSecret: ""

  # scrape interval for grafana datasource
  # set in .Values.grafana.datasources[0].jsonData.timeInterval
  scrapeInterval: 30s

  # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file
  datasources:
    datasources:
      - name: Prometheus
        url: http://{{.Release.Name}}-promxyuser.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8082
        isDefault: true
        orgId: 1
        access: proxy
        type: prometheus
        jsonData:
          timeInterval: "{{.Values.grafana.scrapeInterval}}"

  dataPath: /var/lib/grafana/data

  persistence:
    enabled: true

    ## Persistent Volume storage class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
    storageClass: ""

    ## Persistent Volume access modes
    accessModes:
      - ReadWriteOnce

    ## Persistent Volume size
    size: 8Gi

    ## When set, will use the existing PVC for persistence
    existingClaim: ""

  ## 'volumePermissions' init container parameters
  ## Changes the owner and group of the persistent volume mount point to runAsUser:fsGroup values
  ##   based on the *podSecurityContext/*containerSecurityContext parameters
  volumePermissions:
    enabled: true
    image:
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r378
      pullPolicy: IfNotPresent
    resources: {}

## Asserts knowledge-sensor configuration
##
knowledge-sensor:
  image:
    repository: asserts/knowledge-sensor
    tag: v1.1.9

  serviceAccount:
    create: false
    name: asserts

  initContainers:
    - name: wait-for-asserts-server
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Release.Name}}-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8030"
        - "-t"
        - "420"

  ## tenant and asserts host knowledge sensor is retrieving configurations for
  assertsTenant: bootstrap
  assertsControllerHost: "http://{{.Release.Name}}-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8030"

  ## how often the knowledge sensor will check for configurations
  syncInterval: "30"

  ## The configmap names and target directories for the rules
  ## and relabel rules the configmaps will contain
  ## NOTE: these should not need to change
  prometheusRulesConfigmapName: "asserts-rules"
  prometheusRulesTargetDir: "/opt/asserts/rules"
  prometheusRelabelRulesConfigmapName: "asserts-relabel-rules"
  prometheusRelabelRulesTargetDir: /opt/asserts/relabel

  # enables promxy api and bootstraps promxyruler/promxyuser configmaps
  promxyEnabled: true


## Tsdb configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/victoria-metrics-single/values.yaml
tsdb:
  enabled: true
  rbac:
    create: false
    pspEnabled: false

  serviceAccount:
    create: false
    name: asserts

  configMap: asserts-tsdb-scrapeconfig
  server:
    image:
      tag: v1.75.1

    nameOverride: "tsdb"
    fullNameOverride: "tsdb"

    initContainers:
      ## wait for the knowledge-sensor to be ready to
      ## serve up the rules
      - name: wait-for-knowledge-sensor
        image: asserts/wait-for:v2.2.3
        imagePullPolicy: IfNotPresent
        args:
          - "{{.Release.Name}}-knowledge-sensor.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8080"
          - "-t"
          - "420"
      ## initially add rule files to volume on startup
      ## after the knowledge-sensor is ready. This is a
      ## way to make sure the tsdb doesn't startup with
      ## empty rules as it will crash or run with an empty
      ## set.
      - name: init-add-rule-files
        image: kiwigrid/k8s-sidecar:1.21.0
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-relabel-config
          - name: FOLDER
            value: /opt/asserts/relabel
          - name: METHOD
            value: LIST
        volumeMounts:
          - name: relabel-config
            mountPath: /opt/asserts/relabel
      ## initially add scrape config to volume on startup
      ## This is a way to make sure the tsdb doesn't startup with
      ## an empty scrape config as it will crash in that scenario
      - name: init-add-scrape-config
        image: kiwigrid/k8s-sidecar:1.21.0
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-scrape-config
          - name: FOLDER
            value: /opt/asserts/scrape
          - name: METHOD
            value: LIST
        volumeMounts:
          - name: scrape-config
            mountPath: /opt/asserts/scrape

    extraArgs:
      loggerFormat: default
      relabelConfig: /opt/asserts/relabel/bootstrap.yml
      maxLabelsPerTimeseries: 60
      retentionPeriod: 30d
      search.maxStalenessInterval: 60s
      search.latencyOffset: 15s
      memory.allowedPercent: 50

    ## scrape configuration for the tsdb in config/tsdb-scrape-configmap.yaml
    scrape:
      enabled: true
      configDir: /opt/asserts/scrape
      configPath: /opt/asserts/scrape/scrape.yml
      configMap: asserts-tsdb-scrapeconfig
      extraLabels:
        bootstrap-scrape-config: "1"

    persistentVolume:
      size: 8Gi

    resources: {}

    extraContainers:
      - name: relabel-config-sidecar
        image: kiwigrid/k8s-sidecar:1.21.0
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-relabel-config
          - name: FOLDER
            value: /opt/asserts/relabel
          - name: REQ_URL
            value: http://localhost:8428/-/reload
        volumeMounts:
          - name: relabel-config
            mountPath: /opt/asserts/relabel
      - name: scrape-config-sidecar
        image: kiwigrid/k8s-sidecar:1.21.0
        imagePullPolicy: IfNotPresent
        env:
          - name: LABEL
            value: bootstrap-scrape-config
          - name: FOLDER
            value: /opt/asserts/scrape
          - name: REQ_URL
            value: http://localhost:8428/-/reload
        volumeMounts:
          - name: scrape-config
            mountPath: /opt/asserts/scrape

    extraVolumeMounts:
      - name: relabel-config
        mountPath: /opt/asserts/relabel
      - name: scrape-config
        mountPath: /opt/asserts/scrape

    extraVolumes:
      - name: relabel-config
        emptyDir: {}
      - name: scrape-config
        emptyDir: {}


## RedisGraph configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/redis/values.yaml
redisgraph:
  enabled: true

  ## custom Redis multi module image
  ## ref: https://hub.docker.com/repository/docker/asserts/redismod
  image:
    repository: asserts/redismod
    tag: 6.26

  nameOverride: asserts-redisgraph
  fullnameOverride: asserts-redisgraph

  serviceAccount:
    create: false
    name: asserts

  architecture: standalone

  auth:
    enabled: false

  master:
    configuration: |
      loadmodule /opt/bitnami/redis/modules/redisgraph.so

  sentinel:
    enabled: false

  ## TODO: since we are scraping from the tsdb, may want to disable
  ##       the annotation -> prometheus.io/scrape: true
  ##       so that there isn't a double scrape from another prom
  metrics:
    enabled: true


## RedisSearch configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/redis/values.yaml
redisearch:
  enabled: true

  ## custom Redis multi module image
  ## ref: https://hub.docker.com/repository/docker/asserts/redismod
  image:
    repository: asserts/redismod
    tag: 6.26

  nameOverride: asserts-redisearch
  fullnameOverride: asserts-redisearch

  serviceAccount:
    create: false
    name: asserts

  architecture: standalone

  auth:
    enabled: false

  master:
    configuration: |
      loadmodule /opt/bitnami/redis/modules/redisearch.so

  sentinel:
    enabled: false

  # TODO: since we are scraping from the tsdb, may want to disable
  #       the annotation -> prometheus.io/scrape: true
  #       so that there isn't a double scrape from another prom
  metrics:
    enabled: true


## Alertmanager configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/alertmanager/values.yaml
alertmanager:
  enabled: true

  serviceAccountName: asserts
  serviceAccount:
    create: false
    name: asserts

  persistence:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 100Mi

  existingConfigMap: asserts-alertmanager

  configmapReload:
    enabled: true

## Promxy ruler configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/promxy/values.yaml
promxyruler:
  enabled: true
  image:
    repository: asserts/promxy
    tag: v0.1.0

  serviceAccount:
    create: false
    name: asserts

  existingConfigMap: asserts-promxyruler

  initContainers:
    - name: wait-for-tsdb
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Release.Name}}-tsdb-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8428"
        - "-t"
        - "420"

  extraContainers:
    - name: rules-config-sidecar
      image: kiwigrid/k8s-sidecar:1.21.0
      imagePullPolicy: IfNotPresent
      env:
        - name: LABEL
          value: bootstrap-rules-config
        - name: FOLDER
          value: /opt/asserts/rules
        - name: REQ_URL
          value: http://localhost:8082/-/reload
        - name: REQ_METHOD
          value: POST
      volumeMounts:
        - name: rules-config
          mountPath: /opt/asserts/rules

  extraVolumeMounts:
    - name: rules-config
      mountPath: /opt/asserts/rules
    - name: extra-rules
      mountPath: /opt/asserts/extra-rules/asserts-k8s-calls.yml
      subPath: asserts-k8s-calls.yml

  extraVolumes:
    - name: rules-config
      emptyDir: {}
    - name: extra-rules
      configMap:
        name: asserts-k8s-calls-rules
        items:
          - key: asserts-k8s-calls.yml
            path: asserts-k8s-calls.yml


## Promxy user configuration
## ref: https://github.com/asserts/helm-charts/blob/master/charts/promxy/values.yaml
promxyuser:
  enabled: true
  image:
    repository: asserts/promxy
    tag: v0.1.0

  serviceAccount:
    create: false
    name: asserts

  existingConfigMap: asserts-promxyuser

  initContainers:
    - name: wait-for-tsdb
      image: asserts/wait-for:v2.2.3
      imagePullPolicy: IfNotPresent
      args:
        - "{{.Release.Name}}-tsdb-server.{{.Release.Namespace}}.{{.Values.clusterDomain}}:8428"
        - "-t"
        - "420"


## Postgres configuration
## ref: https://github.com/bitnami/charts/blob/master/bitnami/postgresql/values.yaml
postgres:
  enabled: true

  ## Postgres password configuration
  ##
  ## If left with postgresPassword: "" and existingSecret: ""
  ## a random alpha numeric password will be generated on
  ## first run. Upon upgrading the release, the credentials
  ## stored in the secret will be retrieved, secret handling
  ## is out of the box. Note that this will fail if using the
  ## "helm-diff" plugin and would require setting it explicitly
  ## or providing an existingSecret.
  ##
  ## ref: https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases
  global:
    postgresql:
      auth:
        database: "asserts"
        postgresPassword: ""
        existingSecret: ""

  image:
    repository: bitnami/postgresql
    tag: 12.7.0

  nameOverride: "asserts-postgres"
  fullnameOverride: "asserts-postgres"

  serviceAccount:
    create: false
    name: asserts

  primary:
    initdb:
      scripts:
        load-extensions.sh: |
          #!/bin/sh

          psql --username "postgres" <<EOF
          create extension IF NOT EXISTS pg_stat_statements;
          select * FROM pg_extension;
          EOF

  metrics:
    enabled: true
    service:
      annotations:
        # set to false since this chart is handling scraping for this service
        # the Asserts ServiceMonitor is used or the Asserts TSDB is already scraping this endpoint
        prometheus.io/scrape: "false"
