---
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
{{- if .Values.global.commonAnnotations }}
{{- include "render-value" ( dict "value" .Values.global.commonAnnotations "context" .) | nindent 4 }}
{{- end }}
  labels:
{{- if .Values.global.commonLabels }}
{{- include "render-value" ( dict "value" .Values.global.commonLabels "context" .) | nindent 4 }}
{{- end }}
    app.kubernetes.io/part-of: federatorai
    app: alameda
  name: federatorai-data-adapter-historical-config
  namespace: {{ .Release.Namespace }}
data:
  telegraf_historical.conf: |+
    [global_tags]

    [agent]
      interval = "1m"
      delay_query_interval = "$DELAY_QUERY_INTERVAL"
      delay_historical_query = "$DELAY_HISTORICAL_QUERY"
      round_interval = true
      metric_batch_size = 100000
      metric_buffer_limit = 100000
      collection_jitter = "5s"
      flush_interval = "20s"
      flush_jitter = "0s"
      precision = "1us"
      debug = $DEBUG
      aggregator_queue = 200000
      max_rpc_receive_size = $MAX_RPC_RECEIVE_SIZE
      logfile = "/var/log/telegraf_historical.log"
      logfile_rotation_interval = "1d"
      logfile_rotation_max_archives = 10
      logfile_rotation_max_size = "200MB"
      logfile_compress_archives = true
      logfile_total_max_size = "$FEDERATORAI_MAXIMUM_LOG_SIZE"
      logfile_queue_size = $MAX_LOG_QUEUE_SIZE
      logfile_queue_trace_interval = "$LOG_QUEUE_TRACE_INTERVAL"
      logfile_enabled_async_logger = $ENABLED_ASYNC_LOGGER
      ## Data adapter will flush log queue to console if queue is full
      enable_logfile_flush_to_console = $ENABLE_FLUSH_LOG_TO_CONSOLE
      logfile_flush_to_console_level = "$LOG_FLUSH_TO_CONSOLE_LEVEL"
      ## Query will retry if some metrics hasn't returned yet.
      max_retry = 1
      retry_interval = "10s"
      max_request_line = $MAX_REQUEST_LINE
      sysdig_max_char_per_chunk = $SYSDIG_MAX_CHAR_PER_CHUNK
      # sysdig_max_query_per_chunk accepted value [1-20], default value is 10 if not setted
      sysdig_max_query_per_chunk = $SYSDIG_MAX_QUERY_PER_CHUNK
      quiet = false
      hostname = ""
      omit_hostname = false
      alamedascaler_enable = true
      enable_historical_data_collection = true
      force_reload_configuration = false
      fed_rest_url = "$FED_REST_URL"
      fed_rest_port = "$FED_REST_PORT"
      datahub_url = "$DATAHUB_URL"
      datahub_port = "$DATAHUB_PORT"
      rabbitmq_url = "$RABBITMQ_URL"
      rabbitmq_port = "$RABBITMQ_PORT"
      rabbitmq_subscriber = "historical_data_adapter"
      expired_time = "8760h"
      cloud_metadata_expired_time = "$CLOUD_METADATA_EXPIRED_TIME"
      event_cache_expired_time = "$POST_EVENT_INTERVAL"

      ## set collect_metadata_only=false to collect resource metadata and metrics
      ## set collect_metadata_only=true to only collect resource metadata
      collect_metadata_only = false

      aws_metric_list = "$AWS_METRIC_LIST"
      drop_duplicated_rest_api = true
      force_send_rest_api_time = "1h"

      # Default disable integration metrics setting
      # Cost Analysis related metrics
      cost_analysis_metrics = ["federatorai.cost_analysis.instance.cost","federatorai.cost_analysis.namespace.cost","federatorai.prediction.namespace.cost","federatorai.recommendation.instance","federatorai.cost_analysis.resource_alloc_cost.cluster","federatorai.cost_analysis.resource_alloc_cost.node","federatorai.cost_analysis.resource_alloc_cost.namespace","federatorai.cost_analysis.resource_usage_cost.cluster","federatorai.cost_analysis.resource_usage_cost.node","federatorai.cost_analysis.resource_usage_cost.namespace","federatorai.cost_analysis.cost_per_day.cluster","federatorai.cost_analysis.cost_per_day.node","federatorai.cost_analysis.cost_per_day.namespace","federatorai.cost_analysis.cost_per_week.cluster","federatorai.cost_analysis.cost_per_week.node","federatorai.cost_analysis.cost_per_week.namespace","federatorai.cost_analysis.cost_per_month.cluster","federatorai.cost_analysis.cost_per_month.node","federatorai.cost_analysis.cost_per_month.namespace","federatorai.recommendation.cost_analysis.cost_per_day.cluster","federatorai.recommendation.cost_analysis.cost_per_day.node","federatorai.recommendation.cost_analysis.cost_per_day.namespace","federatorai.recommendation.cost_analysis.cost_per_week.cluster","federatorai.recommendation.cost_analysis.cost_per_week.node","federatorai.recommendation.cost_analysis.cost_per_week.namespace","federatorai.recommendation.cost_analysis.cost_per_month.cluster","federatorai.recommendation.cost_analysis.cost_per_month.node","federatorai.recommendation.cost_analysis.cost_per_month.namespace","federatorai.prediction.cost_analysis.cost_per_day.cluster","federatorai.prediction.cost_analysis.cost_per_day.node","federatorai.prediction.cost_analysis.cost_per_day.namespace","federatorai.prediction.cost_analysis.cost_per_week.cluster","federatorai.prediction.cost_analysis.cost_per_week.node","federatorai.prediction.cost_analysis.cost_per_week.namespace","federatorai.prediction.cost_analysis.cost_per_month.cluster","federatorai.prediction.cost_analysis.cost_per_month.node","federatorai.prediction.cost_analysis.cost_per_month.namespace"]

      ## Automation
      enable_dump_mock_data = false
      testing_mode = false
      fake_fed_server_mode = false
      configuration_mock_data_path = "/var/log/mock_server/mock_data/configuration"
      aws_mock_data_path = "/var/log/mock_server/mock_data/aws"
      azure_mock_data_path = "/var/log/mock_server/mock_data/azure"
      gcp_mock_data_path = "/var/log/mock_server/mock_data/gcp"
      datadog_mock_data_path = "/var/log/mock_server/mock_data/datadog"
      sysdig_mock_data_path = "/var/log/mock_server/mock_data/sysdig"
      prometheus_mock_data_path = "/var/log/mock_server/mock_data/prometheus"
      expected_result_path = "/var/log/mock_server/mock_data/expected_result"
      test_mode_result_path = "mock_server/mock_data/test_mode_result"

      ## Dashboard
      check_sysdig_dashboard_interval = "$CHECK_SYSDIG_DASHBOARD_INTERVAL"
      enable_sysdig_dashboard = $ENABLE_SYSDIG_DASHBOARD
      sysdig_dashboards = ["/etc/telegraf/dashboards/sysdig/kafka-overview.json", "/etc/telegraf/dashboards/sysdig/application-overview.json", "/etc/telegraf/dashboards/sysdig/cluster-overview.json"]

      ## Datadog tag mapping table
      # if enable_autodiscover_datadog_cluster_name_tag_key=true, agent uses datadog_default_cluster_tag_key to autodiscover cluster name tag key
      # if enable_autodiscover_datadog_cluster_name_tag_key=false, agent uses the cluster name tag key defined in agent.datadog_cluster_tag_mapping.
      enable_autodiscover_datadog_cluster_name_tag_key = true
      #DD_CLUSTER_NAME_TAG_KEYS is "cluster_name,kube_cluster_name,kube_cluster" by default.
      datadog_default_cluster_tag_keys  = "$DD_CLUSTER_NAME_TAG_KEYS"
      [agent.datadog_cluster_tag_mapping]
        # Datadog cluster name tag = ["<cluster_name>"]
        kube_cluster = []
        cluster_name = []
        kube_cluster_name = []

    ##################
    ### Processors ###
    ##################

    [[processors.filter_out]]
      order = 1
      namepass = ["kafka_topic_partition_current_offset", "kafka_consumer_group_current_offset", "node_disk_io_util", "sysdig_kafka_topic_partition_current_offset", "sysdig_kafka_consumergroup_current_offset", "prometheus_kube_pod_owner", "prometheus_kube_replicationcontroller_spec_replicas", "prometheus_kube_replicationcontroller_status_available_replicas", "prometheus_controller_replicas", "datadog_node_pod_phase"]
      [[processors.filter_out.metric]]
        measurement_name = "sysdig_kafka_topic_partition_current_offset"
        fields = ["kafka_topic_partition_current_offset"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "sysdig_kafka_consumergroup_current_offset"
        fields = ["kafka_consumergroup_current_offset"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "kafka_topic_partition_current_offset"
        fields = ["value"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "kafka_consumer_group_current_offset"
        fields = ["value"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "node_disk_io_util"
        fields = ["value"]
        aggregation_method = "set_max_value"
        max_value = 100.0
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_pod_owner"
        fields = ["owner_name"]
        aggregation_method = "trim_last_index"
        match_string = "-"
        [processors.filter_out.metric.condition]
          owner_kind = "ReplicaSet"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_pod_owner"
        fields = ["owner_name"]
        aggregation_method = "trim_last_index"
        match_string = "-"
        [processors.filter_out.metric.condition]
          owner_kind = "ReplicationController"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_replicationcontroller_spec_replicas"
        fields = ["value"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_replicationcontroller_status_available_replicas"
        fields = ["value"]
        aggregation_method = "drop_zero_value"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_replicationcontroller_spec_replicas"
        fields = ["replicationcontroller"]
        aggregation_method = "trim_last_index"
        match_string = "-"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_kube_replicationcontroller_status_available_replicas"
        fields = ["replicationcontroller"]
        aggregation_method = "trim_last_index"
        match_string = "-"
      [[processors.filter_out.metric]]
        measurement_name = "prometheus_controller_replicas"
        fields = ["replicationcontroller"]
        aggregation_method = "trim_last_index"
        match_string = "-"

    ## Datadog integration: processors
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["datadog_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/datadog_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["datadog_*", "alameda_*"]
      namedrop = ["datadog_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/datadog_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["datadog_namespace_*"]
      schema_file = "/etc/telegraf/schema/datadog_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["datadog_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/datadog_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "datadog"
      aggregator_processor = true
      namepass = ["datadog_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/datadog_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ## Federation Prometheus/Prometheus integration: processors ##
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["prometheus_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/prometheus_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["prometheus_*", "alameda_*"]
      namedrop = ["prometheus_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/prometheus_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["prometheus_alameda_config_cluster_namespace", "prometheus_node_cpu_usage_seconds_total", "prometheus_node_memory_usage_bytes", "prometheus_kube_node_status_capacity_cpu_cores", "prometheus_namespace_*"]
      schema_file = "/etc/telegraf/schema/prometheus_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["prometheus_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/prometheus_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "prometheus"
      aggregator_processor = true
      namepass = ["prometheus_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/prometheus_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ## Sysdig integration: processors ##
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["sysdig_*", "alameda_*", "federatorai.kafka.consumer_lag", "federatorai.kafka.consumer_offset_rate", "federatorai.kafka.broker_offset_rate", "federatorai.prediction.kafka", "federatorai.recommendation",  "federatorai.prediction.controller", "federatorai.prediction.controller.max", "federatorai.prediction.controller.min", "federatorai.prediction.controller.avg", "federatorai.prediction.node", "federatorai.prediction.node.max", "federatorai.prediction.node.min", "federatorai.prediction.node.avg", "federatorai.resource_planning.node", "federatorai.resource_planning.controller", "federatorai.kubernetes.cpu.usage.total.node", "federatorai.kubernetes.cpu.usage.total.controller", "federatorai.kubernetes.memory.usage.node", "federatorai.kubernetes.memory.usage.controller", "federatorai.kubernetes.cpu.usage.total.node_rollup_3600sec", "federatorai.kubernetes.cpu.usage.total.controller_rollup_3600sec", "federatorai.kubernetes.memory.usage.node_rollup_3600sec", "federatorai.kubernetes.memory.usage.controller_rollup_3600sec", "federatorai.kubernetes.cpu.usage.total.node_rollup_21600sec", "federatorai.kubernetes.cpu.usage.total.controller_rollup_21600sec", "federatorai.kubernetes.memory.usage.node_rollup_21600sec", "federatorai.kubernetes.memory.usage.controller_rollup_21600sec", "federatorai.kubernetes.cpu.usage.total.node_rollup_86400sec", "federatorai.kubernetes.cpu.usage.total.controller_rollup_86400sec", "federatorai.kubernetes.memory.usage.node_rollup_86400sec", "federatorai.kubernetes.memory.usage.controller_rollup_86400sec"]
      schema_file = "/etc/telegraf/schema/sysdig_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["sysdig_*", "alameda_*"]
      namedrop = ["sysdig_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/sysdig_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["sysdig_alameda_config_cluster_namespace", "sysdig_namespace_*", "sysdig_kube_container_resource_limits_cpu_cores", "sysdig_kube_container_resource_requests_cpu_cores", "sysdig_alameda_config_namespace_metric_instance_config_info"]
      schema_file = "/etc/telegraf/schema/sysdig_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["sysdig_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/sysdig_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "sysdig"
      aggregator_processor = true
      namepass = ["sysdig_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/sysdig_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ## VMware integration: processors
    #[[processors.metrics_decorator]]
    #  order = 2
    #  namepass = ["vmware_*", "alameda_*"]
    #  schema_file = "/etc/telegraf/schema/vmware_pre_metric_decorator.json"

    #[[processors.metrics_grouping]]
    #  order = 3
    #  namepass = ["vmware_*", "alameda_*"]
    #  namedrop = ["vmware_historical_collection_done"]
    #  schema_file = "/etc/telegraf/schema/vmware_grouping_rules.json"

    #[[processors.metrics_decorator]]
    #  order = 4
    #  namepass = [""]
    #  schema_file = "/etc/telegraf/schema/vmware_post_metric_decorator.json"

    #[[processors.aggregator]]
    #  order = 5
    #  namepass = ["vmware_*"]
    #  concurrent_job = 3
    #  schema_file = "/etc/telegraf/schema/vmware_aggregator_schema.json"
    #  config_file = "/etc/telegraf/schema/aggregator_config.json"

    #[[processors.schema_mapping]]
    #  order = 6
    #  datasource = "vmware"
    #  aggregator_processor = true
    #  namepass = ["vmware_*", "alameda_*"]
    #  schema_file = "/etc/telegraf/schema/vmware_schema.json"
    #  cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
    #  ## Set default cloud information ifneeded
    #  enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
    #  default_provider = "$DEFAULT_PROVIDER"
    #  default_region = "$DEFAULT_REGION"
    #  default_instance_type = "$DEFAULT_INSTANCE_TYPE"
    #  default_instance_id = "$DEFAULT_INSTANCE_ID"
    #  default_zone = "$DEFAULT_ZONE"

    ## AWS Cloudwatch integration: processors
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["aws_cloudwatch_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/aws_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["aws_cloudwatch_*", "alameda_*"]
      namedrop = ["aws_cloudwatch_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/aws_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["aws_cloudwatch_alameda_config_cluster_vm", "aws_cloudwatch_node_group_*"]
      schema_file = "/etc/telegraf/schema/aws_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["aws_cloudwatch_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/aws_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "aws"
      aggregator_processor = true
      namepass = ["aws_cloudwatch_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/aws_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ## Azure integration: processors
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["azure_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/azure_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["azure_*", "alameda_*"]
      namedrop = ["azure_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/azure_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["azure_alameda_config_cluster_vm", "azure_alameda_config_cluster_scale_set_vm", "azure_node_group_*"]
      schema_file = "/etc/telegraf/schema/azure_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["azure_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/azure_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "azure"
      aggregator_processor = true
      namepass = ["azure_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/azure_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ## GCP integration: processors
    [[processors.metrics_decorator]]
      order = 2
      namepass = ["gcp_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/gcp_pre_metric_decorator.json"

    [[processors.metrics_grouping]]
      order = 3
      namepass = ["gcp_*", "alameda_*"]
      namedrop = ["gcp_historical_collection_done"]
      schema_file = "/etc/telegraf/schema/gcp_grouping_rules.json"

    [[processors.metrics_decorator]]
      order = 4
      namepass = ["gcp_alameda_config_cluster_vm", "gcp_node_group_*"]
      schema_file = "/etc/telegraf/schema/gcp_post_metric_decorator.json"

    [[processors.aggregator]]
      order = 5
      namepass = ["gcp_*"]
      concurrent_job = 3
      schema_file = "/etc/telegraf/schema/gcp_aggregator_schema.json"
      config_file = "/etc/telegraf/schema/aggregator_config.json"

    [[processors.schema_mapping]]
      order = 6
      datasource = "gcp"
      aggregator_processor = true
      namepass = ["gcp_*", "alameda_*"]
      schema_file = "/etc/telegraf/schema/gcp_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
      ## Set default cloud information ifneeded
      enable_set_default_cloud_info_if_empty = $ENABLE_SET_DEFAULT_CLOUD_INFO_IF_EMPTY
      default_provider = "$DEFAULT_PROVIDER"
      default_region = "$DEFAULT_REGION"
      default_instance_type = "$DEFAULT_INSTANCE_TYPE"
      default_instance_id = "$DEFAULT_INSTANCE_ID"
      default_zone = "$DEFAULT_ZONE"

    ############################
    ### Input/Output Plugins ###
    ############################

    ## Alameda Configuration: inputs.data_collector ##
    [[inputs.data_collector]]
      alias = "alameda_configuration_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0h" #Support s(second),m(minute),h(hour)
      collection_jitter = "0s"
      ## data source type from which to query data
      ## accept values: alameda_datahub
      source = "alameda_datahub"
      ## which collector to handle the data collection
      collector = "alameda_datahub"
      ## authenticated token path
      token= "/var/run/secrets/kubernetes.io/serviceaccount/token"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## Alameda Read Configuration used
      configuration_only = true
      overwrite_query_delay_interval = true
      delay_query_interval = "0s"
      ## one URL from which to read formatted metrics
      url = ""
      ## Load metric instance configs from Datahub or not
      skip_metric_instance_configs = true
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/alameda_config_historical_metrics.json"]
      target_label = ""
      cluster_name = "default"
      discover_path = ""
      node_uids = []
      group_names = []

    ## Federation Prometheus integration: inputs.data_collector ##
    [[inputs.data_collector]]
      alias = "federation_prometheus_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: prometheus
      source = "prometheus"
      ## which collector to handle the data collection
      collector = "prometheus"
      ## account name
      account = ""
      ## authenticated token path
      token= "/var/run/secrets/kubernetes.io/serviceaccount/token"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = "http://prometheus-prometheus-oper-prometheus.default.svc:9090"
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/federation_prometheus_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_prometheus_config.json"
      target_label = "clusterID: cluster1"
      cluster_name = "default"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Rancher Prometheus integration: inputs.data_collector ##
    [[inputs.data_collector]]
      alias = "rancher_prometheus_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: prometheus
      source = "prometheus"
      ## which collector to handle the data collection
      collector = "prometheus"
      ## account name
      account = ""
      ## authenticated token path
      token= "/var/run/secrets/kubernetes.io/serviceaccount/token"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = "http://prometheus-prometheus-oper-prometheus.default.svc:9090"
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/rancher_prometheus_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_prometheus_config.json"
      target_label = ""
      cluster_name = "default"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Prometheus integration: inputs.data_collector ##
    [[inputs.data_collector]]
      alias = "prometheus_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: prometheus
      source = "prometheus"
      ## which collector to handle the data collection
      collector = "prometheus"
      ## account name
      account = ""
      ## authenticated token path
      token= "/var/run/secrets/kubernetes.io/serviceaccount/token"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = "http://prometheus-prometheus-oper-prometheus.default.svc:9090"
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/prometheus_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_prometheus_config.json"
      target_label = ""
      cluster_name = "default"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Datadog integration: inputs.data_collector
    [[inputs.data_collector]]
      alias = "datadog_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: prometheus, datadog, sysdig
      source = "datadog"
      ## which collector to handle the data collection
      collector = "datadog"
      ## authenticated token path
      token= "${DD-API-KEY}"
      application_key = "${DD-APPLICATION-KEY}"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = "https://api.datadoghq.com/api/v1/query"
      ## Set batch query parameters
      max_query_per_chunk = $DATADOG_MAX_QUERY_PER_CHUNK
      max_characters_per_chunk = $DATADOG_MAX_CHAR_PER_CHUNK
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/datadog_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_datadog_config.json"
      cluster_name = "default"
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Sysdig integration: inputs.data_collector ##
    [[inputs.data_collector]]
      alias = "sysdig_collector"
      query_start_time_offset = "-1m"
      query_end_time_offset = "0m"
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: sysdig
      source = "sysdig"
      ## which collector to handle the data collection
      collector = "sysdig"
      ## account name
      account = ""
      ## authenticated token path
      token= "${SYSDIG_API_TOKEN}"
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = "${SYSDIG_API_URL}/data/batch"
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/sysdig_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_sysdig_config.json"
      target_label = ""
      cluster_name = "${CLUSTER_NAME}"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## VMware integration: inputs.data_collector ##
    #[[inputs.data_collector]]
    #  alias = "vmware_metrics_collector"
    #  #interval = "5m"
    #  query_start_time_offset = "-5m"
    #  query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
    #  retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
    #  max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
    #  ## data source type from which to query data
    #  ## accept values: vmware
    #  source = "vmware"
    #  ## which collector to handle the data collection
    #  collector = "vmware"
    #  ## account name
    #  account = ""
    #  ## authenticated token path
    #  token= ""
    #  ## TLS Insecure skip verify
    #  insecure_skip_verify = true
    #  ## one URL from which to read formatted metrics
    #  url = "${VMWARE_API_URL}"
    #  ## metrics schema path
    #  metric_path = ["/etc/telegraf/schema/vmware_historical_metrics.json"]
    #  config_file = "/etc/telegraf/schema/collector_vmware_config.json"
    #  target_label = ""
    #  cluster_name = "${CLUSTER_NAME}"
    #  discover_path = ""
    #  controller_name = []
    #  node_uids = []
    #  group_names = []
    #  historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
    #  handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES

    ## AWS Cloudwatch integration: inputs.data_collector
    [[inputs.data_collector]]
      alias = "aws_metrics_collector"
      #interval = "5m"
      query_start_time_offset = "-30m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: vmware
      source = "aws"
      ## which collector to handle the data collection
      collector = "aws"
      ## account name
      account = ""
      ## authenticated token path
      token= ""
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = ""
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/aws_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_aws_config.json"
      cluster_name = "${CLUSTER_NAME}"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Azure integration: inputs.data_collector
    [[inputs.data_collector]]
      alias = "azure_metrics_collector"
      #interval = "5m"
      query_start_time_offset = "-10m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: vmware
      source = "azure"
      ## which collector to handle the data collection
      collector = "azure"
      ## account name
      account = ""
      ## authenticated token path
      token= ""
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = ""
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/azure_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_azure_config.json"
      cluster_name = "${CLUSTER_NAME}"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## GCP integration: inputs.data_collector
    [[inputs.data_collector]]
      alias = "gcp_metrics_collector"
      #interval = "5m"
      query_start_time_offset = "-10m"
      query_end_time_offset = "0m" #Support s(second),m(minute),h(hour)
      retry_interval = "$DATASOURCE_METRICS_RETRY_INTERVAL"
      max_retry_times = $DATASOURCE_METRICS_MAX_RETRY_TIMES
      ## data source type from which to query data
      ## accept values: vmware
      source = "gcp"
      ## which collector to handle the data collection
      collector = "gcp"
      ## account name
      account = ""
      ## authenticated token path
      token= ""
      ## TLS Insecure skip verify
      insecure_skip_verify = true
      ## one URL from which to read formatted metrics
      url = ""
      ## metrics schema path
      metric_path = ["/etc/telegraf/schema/gcp_historical_metrics.json"]
      config_file = "/etc/telegraf/schema/collector_gcp_config.json"
      cluster_name = "${CLUSTER_NAME}"
      discover_path = ""
      controller_name = []
      node_uids = []
      group_names = []
      historical_data_restart_limit = $HISTORICAL_DATA_RESTART_LIMIT
      handle_missing_data_times = $HANDLE_MISSING_DATA_TIMES
      stop_when_no_data = $STOP_WHEN_NO_DATA

    ## Federation Prometheus/Prometheus integration: outputs.alameda_datahub ##
    [[outputs.alameda_datahub]]
      namepass = ["prometheus_*"]
      metric_prefix = "prometheus_"
      datasource = "prometheus"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## Sysdig integration: outputs.alameda_datahub ##
    [[outputs.alameda_datahub]]
      namepass = ["sysdig_*"]
      metric_prefix = "sysdig_"
      datasource = "sysdig"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## Datadog integration: outputs.alameda_datahub ##
    [[outputs.alameda_datahub]]
      namepass = ["datadog_*"]
      metric_prefix = "datadog_"
      datasource = "datadog"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## VMware integration: outputs.alameda_datahub ##
    [[outputs.alameda_datahub]]
      namepass = ["vmware_*"]
      metric_prefix = "vmware_"
      datasource = "vmware"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## AWS Cloudwatch integration: outputs.alameda_datahub
    [[outputs.alameda_datahub]]
      namepass = ["aws_cloudwatch_*"]
      metric_prefix = "aws_cloudwatch_"
      datasource = "aws"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## Azure integration: outputs.alameda_datahub
    [[outputs.alameda_datahub]]
      namepass = ["azure_*"]
      metric_prefix = "azure_"
      datasource = "azure"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## GCP integration: outputs.alameda_datahub
    [[outputs.alameda_datahub]]
      namepass = ["gcp_*"]
      metric_prefix = "gcp_"
      datasource = "gcp"
      concurrent_job = $WRITE_DATAHUB_CONCURRENT_JOB
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"

    ## Post Datahub event
    [[outputs.alameda_datahub]]
      namepass = ["datahub_event"]
      metric_prefix = "datahub_"
      post_event_interval = "$POST_EVENT_INTERVAL"
      schema_file = "/etc/telegraf/schema/datahub_historical_schema.json"
      cluster_collection_done_schema_file = "/etc/telegraf/schema/cluster_collection_done_schema.json"
